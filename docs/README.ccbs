
             Introduction to the features of the ccbs framework
            -==================================================-

Implemented goals
 1 migrations and database structure
 2 fixtures
 3 unit tests
 4 library initialization
 5 compilation
 6 deployment

Not yet implemented goals
 7 documentation
 8 functional and system tests


 //====\\ /-----------------------------------\
 || 1. || | migrations and database structure |
 \\====// \-----------------------------------/

Each DBMS has a subdirectory under db/ (e.g. db/progress/).
These should contain further subdirectories:
 db/DBMS/store            - contains the actual data files
 db/DBMS/store/migrations - information about applied migrations
 db/DBMS/store/cache      - cached ascii structure information
 db/DBMS/migrations       - migration scripts
 db/DBMS/fixtures         - initial setup fixtures

The following make targets should be defined:

 directory     target
============= =============================================
 store         create
               start
               stop
               migrate (delegates to migrations directory)
 store/cache   update
 migrations    migrate = default VERSION=xx
               dry_run
               status
 fixtures      load = default

In production mode creation means linking all db-files from a configured
external location (possible another or multiple partitions). In development
mode new generic small databases are created from scratch.

The progress migrations have some additional features:
 * as migration scripts are cached, even those will be downgraded, for
   which the original script vanished (because of a version roleback).
 * a full history of up- and downgrades is kept (target "history").
 * to prevent manual manipulation of the db schema, a differencial check
   is performed before the migration.




 //====\\ /----------\
 || 2. || | fixtures |
 \\====// \----------/

The initial fixtures should setup the database so that the applications can
run. No real e.g. customers or subscriptions are needed, but configuration
data.

Additionally to the initial fixtures in db/DBMS/fixtures, the tests can
use test fixtures. These are in files under the appropriate test subdirectory.

As a third kind, you can define development fixtures anywhere. A suggested
location would be a fixtures subdirectory under the module's root, e.g.
in tms/fixtures. Consider not having them under version control, until you
are sure that more than one person uses them.

All fixtures are written in the YAML format. The following is a description
of special features of the 4gl fixture loading:

 4gl fixture loading
 ~~~~~~~~~~~~~~~~~~~
The yaml file can contain any number of three different elements:

 1. An INCLUDE directive.

{path/to/defines.i}

    The given file is searched for in the propath (which is the sandbox
    at the time of test runs - so this file is also a dependency of the test!)
    and parsed for &GLOBAL-DEFINE statements. The code is not executed and
    also commented defines will be found.

 2. A SEQUENCE definition

name: sequence
  block of code

    The word "sequence" identifies this element. It defines a sequence
    (, which is NOT the same as a database sequence) under the given name.
    When a new sequence value is requested, the block of code will be run
    as a procedure with input parameter n AS INT. The last line of the block
    will be wrapped in "RETURN STRING(<last line>)."
    The first example simply uses an exiting database sequence, while the
    second one returns a semi-random name.

MyCustNum: sequence
  NEXT-VALUE(CustNum)

CustName: sequence
  ENTRY((n MOD 5) + 1, "Tim,Bob,Jim,Jon,Ben")

 3. A fixture generator

name: tablename
  key1: value1
  key2: value2
  key3[1]: extended_value
  ...

    In the simplest case this defines statically one table record for the
    given table. All columns that are not mentioned will receive the
    default value of their structure defintion.
    The value can contain one or more of the following functions:

CustId: next(MyCustNum)       - next() to refer to fixture sequences
Status: {&MS_STAT_ACTIVE}     - global definition from included file
ForeignId: hash(fixturename)  - integer hash value of a string
                                (string defaults to name of current fixture)
Created: NOW - 5              - simple date arithmetic and NOW or TODAY
ForeignId: generate(otherfixture, key4=overridden).key5
                              - recursively generates another fixture,
                                overriding the given attributes and returning
                                the given key as value. This last key
                                defaults to TablenameId of the generated fix.


 //====\\ /------------\
 || 3. || | unit tests |
 \\====// \------------/

All test directories should define the "test" target as default. It will run
all tests and exit with the appropriate error response. All 4gl code is
tested with the tools/funit framework. Check the code files for documentation.

For convenience there are different subclasses:

  UnitTest
    Base class, good for generic use
  ProceduralTest
    Tests functions and procedures inside of an .i file
  ProcTmsrpcTest
    Tests a RPC method of the procedural version family 1.x

The mocking module is currently unused, as there are big difficulties in
automatically generating the procs and classes. Manual mocking works as
described in UnitTest.cls.



 //====\\ /------------------------\
 || 4. || | library initialization |
 \\====// \------------------------/

Before either a checkout or an unpacked distribution can be used, the whole
ccbs needs to be initialized. This will
 a) create a site.make
 b) compile the tools and install dependencies from source
 c) create (or link to) the databases

Although the principle is the same, the action flow is slightly different
on production and development. For production initialization
see chapter [6 deployment].

On invokation of the "make initialize" target in the project root, the 
etc/site.make file will be created. That file contains important definitions
that are used by all makefiles of the ccbs but depend entirely on the
local environment. Most likely the auto-generated file is fine, but you might
want to look at the definitions. During development you can change these
values freely, as that file will not be copied to other developers.

Running the "make initialize" target a second time will execute steps b and
c above: the funit and fixtures tools are pre-compiled and the fcgi_agents
framework is installed into the rpc subdir (if the customer uses the rpc).
The newly created databases will (on development only!) immediately
migrated to the latest version and the initial fixtures will be loaded.
Note that these steps do not happen on production (see below).

Ultimately make should finish with "Initialization successfully finished".
The checkout is marked with the ".initialized" file. If you delete this
file, you can run the initialization again: The database "create" target
will not do anthing but the tools and srcpkgs will be recompiled.



 //====\\ /-------------\
 || 5. || | compilation |
 \\====// \-------------/

This chapter explains the compilation of the TMS.
According to newest research, the .r files are actually not necessary:

 * On development you want to run on the .p files only, so that code changes
   are immediately reflected in runtime. **)

 * During test, the tested files will be compiled, but the .r file will not
   be preserved.

 * On production, we have no source code, so compilation does not happen.
   The .r files are *once* produced during the build of the distribution
   and immediately packed into a library .pl file.

**) Unfortunately RUN-statements without the .p will run only .r files and
ignore the .ps. During development this means that you have to have those
.r files *additionally*, but later in the propath. So the recommended usage
is to mention the .p in RUN statements. Until then, you can build the
distribution .pl file with "make build". The "make run" target mentions this
at the end of the PROPATH.


So compilation is used for two purposes:
 1) produce a distribution
 2) check syntax during development

As for 1): the "make build" target compiles all files in parallel into a
subdir and will archive in .pl + delete them.
For 2) running a complete "make compile" will take inconveniently long.
Instead you can give a specific .r file as target. Still note, that that .r
file will not be actually created:

  make Inv/nnpura.r

This will compile Inv/nnpura.p without the SAVE modifier, i.e. it will fail
on any error in the .p file or any included .i file.
The provim plugin does this on <F2> and even tries to interpret the error
output.


Finally there is usage 3): If you changed a .i file you might want to check
the syntax of all .p files that included your .i (possibly via intermediate
includes). I intend to introduce a dependency-based compilation again, but
am not sure yet how to do it.


 RPC method compilation
 ~~~~~~~~~~~~~~~~~~~~~~

The running TMSRPC currently requires .r files, so the above stated is not
true. The "make compile" target will create the .r files according to
dependencies, but not following the dependencies inside TMS.


 Provim - plugin for vim
 ~~~~~~~~~~~~~~~~~~~~~~~

Copy the contents of tools/provim/vimrc.template into your ~/.vimrc and
more importantly the file tools/provim/progress.vim into ~/.vim/syntax/.
Tha latter will automatically be executed whenever the syntax "progress" is
enabled. Additionally to the syntax highlighting of the original progress.vim
this patched version will descend the directory tree starting from CWD,
trying to find out if it is in a ccbs subdirectory. If so, it will load the
following bindings:

 * <F2> - check syntax ( = compile but do not save = call make)
   The output of make (visible in the shell buffer - call ":sh<enter>")
   will be interpreted and is browsable with :cn and :cp.
   As the output of the progress compiler is not very standard, the
   interpretation might sometimes be wrong.
 * (normal mode) K - keyword program
   Will show the schema definition of the table or sequence under the cursor
 * (insert mode) <ctrl-x><ctrl-o> - omnicompletion
   Will complete table names and fields inside tables.

Further, case-insensitive search will be enabled and the tabulator is
associated with three spaces. The fold-method is "syntax", so that you can
fold (normal mode zc <-> zo) comments and DO blocks.
Normal mode gf (goto file) will open the file under the cursor in a new
buffer. This is possible for tms code files, as these are in the path.

One more file abbrev_maps.vim defines abbreviations and mappings of mikko.
E.g. "dv " in insert mode will expand to "DEFINE VARIABLE " and you can
browse buffers with F6 and F7.
To enable this, un-comment the line close to the end of progress.vim.


 //====\\ /------------\
 || 6. || | deployment |
 \\====// \------------/

Any initialized checkout can create a distribution file with the "make dist"
target in the project root. That distribution contains compiled 4gl code
(but hardly any sources), which will run on any other server with
 * the same progress version
 * the same architecture (32 bit vs. 64 bit)
The fcgi_agents are contained as external source package, so it will be
compiled during initialization on the target server.

Unpacking the resulting <CUSTOMER>-<VERSION>.tar.bz2 file will create a
versioned subdirectory. It's recommended to unpack it in the /apps directory.
On initialization, the package will check for an unversioned link
  /apps/<CUSTOMER> -> /apps/<CUSTOMER>-<CURRENTLY-DEPLOYED-VERSION>
and copy many configuration files from it's etc directory.
If the link does not exist, it will (just like development) generate templates
and stop for manual check. Restart the initialization after checking!

The initialization will compile the external packages and link the databases.
It will NOT run any daemons or execute the migrations.
For this there is a new "activate" target, which will (if necessary) stop
all daemons of the currently running version, re-link /apps/<CUSTOMER> to
itself, run the migrations and re-start the daemons.



 //====\\ /---------------\
 || 7. || | documentation |
 \\====// \---------------/

The place for all generated and static documentation is the docs subdirectory.
It's target "docs" = default will re-generate any non-existent docs:

 * overview (this file)
 * database structure documentation
 * menutree content
 * tms code
 * rpc functions
 * ccbs tools

Best option for 4gl documentation might be robodoc: It does not try to parse
the code but still makes it possible to use actual code lines in the docs.

The "disadvantage" is that non-documented code will never even appear by
function/procedure/class name in the docs. I don't see that as disadvantage,
though, as procedural code does not distinguish between private and public
functions and  has a 50% repetition ratio. In other words: blindly generating
docs for all existing functions will output ~80% garbage. Instead all new
code that is created according to conventions should get documentated.



 //====\\ /-----------------------------\
 || 8. || | functional and system tests |
 \\====// \-----------------------------/

System or integration tests need to be custom made for each pair of systems
that are tested. An example exists in prodigy/test, where 4gl code is used
to create "commlines" that are then parsed by prodigy for validity.
Another already implemented instance are the rpc tests, where the subclass
ProcTmsrpcTest provides the means to call the rpc methods, which in turn
call some TMS functions.

Framework exist for:
 * TMS against prodigy
 * RPC against TMS
Not yet tested:
 * TMS cdr reader against the cdr stream (no cdr simulator yet in ccbs)
 * web against TMS (as there is no web in any ccbs yet)
 * any customer specific 3rd party


Functional or acceptance tests need to be provided by management/customer.
We don't have a tool yet. I'm investigating concordion.
